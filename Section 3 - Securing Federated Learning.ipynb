{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section: Securing Federated Learning\n",
    "\n",
    "- Lesson 1: Trusted Aggregator\n",
    "- Lesson 2: Intro to Additive Secret Sharing\n",
    "- Lesson 3: Intro to Fixed Precision Encoding\n",
    "- Lesson 4: Secret Sharing + Fixed Precision in PySyft\n",
    "- Final Project: Federated Learning wtih Encrypted Gradient Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Federated Learning with a Trusted Aggregator\n",
    "\n",
    "In the last section, we learned how to train a model on a distributed dataset using Federated Learning. In particular, the last project aggregated gradients directly from one data owner to another. \n",
    "\n",
    "However, while in some cases it could be ideal to do this, what would be even better is to be able to choose a neutral third party to perform the aggregation.\n",
    "* a neutral 3rd party who has a machine that we can trust to not look at the gradients when performing the aggregation\n",
    "* we can choose anyone -> the likelihood that we can find is much higher\n",
    "\n",
    "As it turns out, we can use the same tools we used previously to accomplish this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Federated Learning with a Trusted Aggregator\n",
    "\n",
    "When we aggregate our gradients from multiple workers, instead of bringing them to a central server to aggregate them or having them send gradients directly to each other we're goona have everyone who's involved in the federated learning process or the federated learning session send all of their gradients to a neutral third party (secure worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "import torch as th\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the hook\n",
    "hook = sy.TorchHook(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data owners\n",
    "bob = sy.VirtualWorker(hook, id='bob')\n",
    "alice = sy.VirtualWorker(hook, id='alice')\n",
    "# trusted third party\n",
    "secure_worker = sy.VirtualWorker(hook, id='secure_worker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Worker alice already exists. Replacing old worker which could cause                     unexpected behavior\n",
      "WARNING:root:Worker secure_worker already exists. Replacing old worker which could cause                     unexpected behavior\n",
      "WARNING:root:Worker bob already exists. Replacing old worker which could cause                     unexpected behavior\n",
      "WARNING:root:Worker secure_worker already exists. Replacing old worker which could cause                     unexpected behavior\n",
      "WARNING:root:Worker alice already exists. Replacing old worker which could cause                     unexpected behavior\n",
      "WARNING:root:Worker bob already exists. Replacing old worker which could cause                     unexpected behavior\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<VirtualWorker id:secure_worker #tensors:0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inform workers that other workers exist\n",
    "bob.add_workers([alice, secure_worker])\n",
    "alice.add_workers([bob, secure_worker])\n",
    "secure_worker.add_workers([alice, bob])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create dataset for simple linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Toy Dataset\n",
    "data = th.tensor([[0,0],[0,1],[1,0],[1,1.]], requires_grad=True)\n",
    "target = th.tensor([[0],[0],[1],[1.]], requires_grad=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pointers to training data on each worker by\n",
    "# sending some training data to bob and alice\n",
    "# bob data\n",
    "bobs_data = data[0:2].send(bob)\n",
    "bobs_target = target[0:2].send(bob)\n",
    "# alice data\n",
    "alices_data = data[2:].send(alice)\n",
    "alices_target = target[2:].send(alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize toy model\n",
    "model = nn.Linear(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### federated learning\n",
    "\n",
    "* we want to have two different model that we want to send to each of the workers because we want to be averaged\n",
    "* we need two optimizers: one for bob and one for parameters of alice's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob:tensor(0.0383) Alice:tensor(0.0197)\n",
      "Bob:tensor(0.0109) Alice:tensor(0.0017)\n",
      "Bob:tensor(0.0042) Alice:tensor(8.0833e-05)\n",
      "Bob:tensor(0.0021) Alice:tensor(8.5223e-06)\n",
      "Bob:tensor(0.0012) Alice:tensor(5.1591e-05)\n",
      "Bob:tensor(0.0008) Alice:tensor(6.8679e-05)\n",
      "Bob:tensor(0.0005) Alice:tensor(6.5655e-05)\n",
      "Bob:tensor(0.0004) Alice:tensor(5.5286e-05)\n",
      "Bob:tensor(0.0003) Alice:tensor(4.3969e-05)\n",
      "Bob:tensor(0.0002) Alice:tensor(3.4018e-05)\n"
     ]
    }
   ],
   "source": [
    "for round_iter in range(10):\n",
    "\n",
    "    # copy the model and send it to bob\n",
    "    bobs_model = model.copy().send(bob)\n",
    "    alices_model = model.copy().send(alice)\n",
    "\n",
    "    # initialize the optimizers\n",
    "    bobs_opt = optim.SGD(params=bobs_model.parameters(), lr=0.1)\n",
    "    alices_opt = optim.SGD(params=alices_model.parameters(), lr=0.1)\n",
    "\n",
    "    # training the models and average the models\n",
    "\n",
    "    for i in range(10):\n",
    "\n",
    "        # for bob\n",
    "        # zero out the gradients\n",
    "        bobs_model.zero_grad()\n",
    "        # make prediction\n",
    "        bobs_pred = bobs_model(bobs_data)\n",
    "        # calculate the loss with SME \n",
    "        bobs_loss = ((bobs_pred - bobs_target)**2).sum()\n",
    "        # backpropagate\n",
    "        bobs_loss.backward()\n",
    "        # make an optimizer step to update the weights \n",
    "        bobs_opt.step()\n",
    "        # get the loss\n",
    "        bobs_loss = bobs_loss.get().data\n",
    "        #print(bobs_loss)\n",
    "\n",
    "        # for alice\n",
    "        # zero out the gradients\n",
    "        alices_model.zero_grad()\n",
    "        # make prediction\n",
    "        alices_pred = alices_model(alices_data)\n",
    "        # calculate the loss with SME \n",
    "        alices_loss = ((alices_pred - alices_target)**2).sum()\n",
    "        # backpropagate\n",
    "        alices_loss.backward()\n",
    "        # make an optimizer step to update the weights \n",
    "        alices_opt.step()\n",
    "        # get the loss\n",
    "        alices_loss = alices_loss.get().data\n",
    "        #print(alices_loss)\n",
    "\n",
    "    # iterates through every parameter on alices model and call move on that parameter\n",
    "    # move models on secure_worker\n",
    "    alices_model.move(secure_worker)\n",
    "    bobs_model.move(secure_worker)\n",
    "    \n",
    "    with th.no_grad():\n",
    "        # average the models\n",
    "        model.weight.set_(((alices_model.weight.data + bobs_model.weight.data) / 2).get())\n",
    "        model.bias.set_(((alices_model.bias.data + bobs_model.bias.data) / 2).get())\n",
    "        \n",
    "    secure_worker.clear_objects()\n",
    "    \n",
    "    print(\"Bob:\" + str(bobs_loss) + \" Alice:\" + str(alices_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters for bob's and alice's model\n",
    "secure_worker._objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{60587739820: tensor([[0., 0.],\n",
       "         [0., 1.]], requires_grad=True), 4902868227: tensor([[0.],\n",
       "         [0.]], requires_grad=True), 99165568907: tensor([[ 0.0124],\n",
       "         [-0.0075]], grad_fn=<AddmmBackward>)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob._objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{37005961846: tensor([[1., 0.],\n",
       "         [1., 1.]], requires_grad=True), 25998039684: tensor([[1.],\n",
       "         [1.]], requires_grad=True), 40224110750: tensor([[0.9954],\n",
       "         [1.0036]], grad_fn=<AddmmBackward>)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice._objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Intro to Additive Secret Sharing\n",
    "\n",
    "While being able to have a trusted third party to perform the aggregation is certainly nice, in an ideal setting we wouldn't have to trust anyone at all. This is where Cryptography can provide an interesting alterantive. \n",
    "\n",
    "Specifically, we're going to be looking at a simple protocol for Secure Multi-Party Computation called Additive Secret Sharing. This protocol will allow multiple parties (of size 3 or more) to aggregate their gradients without the use of a trusted 3rd party to perform the aggregation. In other words, we can add 3 numbers together from 3 different people without anyone ever learning the inputs of any other actors.\n",
    "\n",
    "Let's start by considering the number 5, which we'll put into a varible x\n",
    "\n",
    "* aggregate these gradients in an encrypted state\n",
    "* no individual person will see those gradients\n",
    "\n",
    "Shared ownership/shared governance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we iterate, the loss will be minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we wanted to SHARE the ownership of this number between two people, Alice and Bob. We could split this number into two shares, 2, and 3, and give one to Alice and one to Bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob_x_share = 2\n",
    "alice_x_share = 3\n",
    "\n",
    "decrypted_x = bob_x_share + alice_x_share\n",
    "decrypted_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that neither Bob nor Alice know the value of x. They only know the value of their own SHARE of x. Thus, the true value of X is hidden (i.e., encrypted). \n",
    "\n",
    "The truly amazing thing, however, is that Alice and Bob can still compute using this value! They can perform arithmetic over the hidden value! Let's say Bob and Alice wanted to multiply this value by 2! If each of them multiplied their respective share by 2, then the hidden number between them is also multiplied! Check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob_x_share = 2 * 2\n",
    "alice_x_share = 3 * 2\n",
    "\n",
    "decrypted_x = bob_x_share + alice_x_share\n",
    "decrypted_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This even works for addition between two shared values!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encrypted \"5\"\n",
    "bob_x_share = 2\n",
    "alice_x_share = 3\n",
    "\n",
    "# encrypted \"7\"\n",
    "bob_y_share = 5\n",
    "alice_y_share = 2\n",
    "\n",
    "# encrypted 5 + 7\n",
    "bob_z_share = bob_x_share + bob_y_share\n",
    "alice_z_share = alice_x_share + alice_y_share\n",
    "\n",
    "decrypted_z = bob_z_share + alice_z_share\n",
    "decrypted_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we just added two numbers together while they were still encrypted!!!\n",
    "\n",
    "One small tweak - notice that since all our numbers are positive, it's possible for each share to reveal a little bit of information about the hidden value, namely, it's always greater than the share. Thus, if Bob has a share \"3\" then he knows that the encrypted value is at least 3.\n",
    "\n",
    "This would be quite bad, but can be solved through a simple fix. Decryption happens by summing all the shares together MODULUS some constant. I.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23740629843736686616461"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 5\n",
    "\n",
    "Q = 23740629843760239486723\n",
    "\n",
    "bob_x_share = 23552870267 # <- a random number\n",
    "alice_x_share = Q - bob_x_share + x\n",
    "alice_x_share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bob_x_share + alice_x_share) % Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, as you can see, both shares are wildly larger than the number being shared, meaning that individual shares no longer leak this inforation. However, all the properties we discussed earlier still hold! (addition, encryption, decryption, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Build Methods for Encrypt, Decrypt, and Add \n",
    "\n",
    "In this project, you must take the lessons we learned in the last section and write general methods for encrypt, decrypt, and add. Store shares for a variable in a tuple like so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define ```encrypt()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_share = (2,5,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though normally those shares would be distributed amongst several workers, you can store them in ordered tuples like this for now :)\n",
    "\n",
    "```sum(shares) % Q = x```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input a tuple, x and number of shares\n",
    "# sum the shares toghether and take a modulus of that sum\n",
    "# it will descrypt to the correct value\n",
    "def encrypt(x, n_shares=3):\n",
    "    \n",
    "    shares = list()\n",
    "\n",
    "    # generate the first two shares randomly\n",
    "    for i in range(n_shares - 1):\n",
    "        shares.append(random.randint(0, Q))\n",
    "\n",
    "    final_share = Q - (sum(shares) % Q) + x\n",
    "\n",
    "    shares.append(final_share)\n",
    "\n",
    "    return tuple(shares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 23740629843760239486723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21597179522291401762004,\n",
       " 6416049415504118228326,\n",
       " 14841024364321363973762,\n",
       " 7947195663863917936334,\n",
       " 21100714859057051088285,\n",
       " 3741620568934529596861,\n",
       " 22563181941837810698297,\n",
       " 13571906918269134215161,\n",
       " 21186138546649603116508,\n",
       " 9478767261832506304805)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encrypt(5, n_shares=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define ```decrypt()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt(shares):\n",
    "    return sum(shares) % Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decrypt(encrypt(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define ```add()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accepts two different tuples\n",
    "def add(a, b):\n",
    "    \n",
    "    c = list()\n",
    "    \n",
    "    assert(len(a) == len(b))\n",
    "    \n",
    "    for i in range(len(a)):\n",
    "        c.append((a[i] + b[i]) % Q)\n",
    "        \n",
    "    return tuple(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decrypt(add(encrypt(5), encrypt(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7117559698689842068389, 4508699778082869664980, 12114370366987527753359),\n",
       " (15339271351164757262471, 7608963749108748285553, 792394743486733938709))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = encrypt(5)\n",
    "y = encrypt(10)\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22456831049854599330860, 12117663527191617950533, 12906765110474261692068)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = add(x,y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decrypt(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Intro to Fixed Precision Encoding\n",
    "\n",
    "As you may remember, our goal is to aggregate gradients using this new Secret Sharing technique. However, the protocol we've just explored in the last section uses positive integers. However, our neural network weights are NOT integers. Instead, our weights are decimals (floating point numbers).\n",
    "\n",
    "Not a huge deal! We just need to use a fixed precision encoding, which lets us do computation over decimal numbers using integers!\n",
    "\n",
    "* PRECISION: how many decimal points we actually want to encode our numbers with\n",
    "* BASE 10 (normal encodyng), 2 (bynary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE=10\n",
    "# 4 decimal values\n",
    "PRECISION=4\n",
    "Q=23740629843760239486723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "    return int((x * (BASE ** PRECISION)) % Q)\n",
    "\n",
    "def decode(x):\n",
    "    return (x if x <= Q/2 else x - Q) / BASE**PRECISION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35000"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.5 means 35000\n",
    "encode(3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(35000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23740629843760239345664, 5e-05)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(-0.5), decode(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(5000 + 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.8"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = encrypt(encode(5.5))\n",
    "y = encrypt(encode(2.3))\n",
    "z = add(x,y)\n",
    "decode(decrypt(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Secret Sharing + Fixed Precision in PySyft\n",
    "\n",
    "While writing things from scratch is certainly educational, PySyft makes a great deal of this much easier for us through its abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob = bob.clear_objects()\n",
    "alice = alice.clear_objects()\n",
    "secure_worker = secure_worker.clear_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data\n",
    "x = th.tensor([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secret Sharing Using PySyft\n",
    "\n",
    "We can share using the simple .share() method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>[AdditiveSharingTensor]\n",
       "\t-> (Wrapper)>[PointerTensor | me:59140615744 -> bob:17438411538]\n",
       "\t-> (Wrapper)>[PointerTensor | me:28503946172 -> alice:28493265064]\n",
       "\t-> (Wrapper)>[PointerTensor | me:38366978111 -> secure_worker:83251700870]\n",
       "\t*crypto provider: me*"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the data into multiple different additive secret shares \n",
    "# then send those shares to bob, alice and secure_worker\n",
    "x = x.share(bob, alice, secure_worker)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{17438411538: tensor([4417300060625948145, 3784897679592191331, 4512047126824704739,\n",
       "         3973372192880975151,  915133640708791685])}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob._objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and as you can see, Bob now has one of the shares of x! Furthermore, we can still call addition in this state, and PySyft will automatically perform the remote execution for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>[AdditiveSharingTensor]\n",
       "\t-> (Wrapper)>[PointerTensor | me:34836591538 -> bob:9053020198]\n",
       "\t-> (Wrapper)>[PointerTensor | me:93091533370 -> alice:64751503372]\n",
       "\t-> (Wrapper)>[PointerTensor | me:26606486632 -> secure_worker:27358104088]\n",
       "\t*crypto provider: me*"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  4,  6,  8, 10])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Precision using PySyft\n",
    "\n",
    "We can also convert a tensor to fixed precision using .fix_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([0.1,0.2,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.2000, 0.3000])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.fix_prec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>FixedPrecisionTensor>tensor([100, 200, 300])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100, 200, 300])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.child.child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "syft.frameworks.torch.tensors.interpreters.native.Tensor"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch tensor\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "syft.frameworks.torch.tensors.interpreters.precision.FixedPrecisionTensor"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is an interpreter\n",
    "type(x.child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "syft.frameworks.torch.tensors.interpreters.native.Tensor"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actual encoding\n",
    "type(x.child.child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.2000, 0.3000])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.float_prec()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.4000, 0.6000])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.float_prec()\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Fixed Precision\n",
    "\n",
    "And of course, we can combine the two!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([0.1, 0.2, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.fix_prec().share(bob, alice, secure_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>FixedPrecisionTensor>(Wrapper)>[AdditiveSharingTensor]\n",
       "\t-> (Wrapper)>[PointerTensor | me:44079632468 -> bob:25617235153]\n",
       "\t-> (Wrapper)>[PointerTensor | me:30051213728 -> alice:17812033504]\n",
       "\t-> (Wrapper)>[PointerTensor | me:15985362009 -> secure_worker:60132834401]\n",
       "\t*crypto provider: me*"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.4000, 0.6000])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.get().float_prec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to make the point that people can see the model averages in the clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Federated Learning with Encrypted Gradient Aggregation\n",
    "\n",
    "* build on the project wuth a trusted secure aggregator\n",
    "* aggregate gradients using additive secret sharing and fixed precision encoding\n",
    "* use three data owners per aggregation\n",
    "* replace the trusted third part with this encryption protocol (additive secret sharing protocol) so that no one actually has to share their own gradients with any other worker directly\n",
    "* instead, they will encrypt the individual values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clear objects on workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob = bob.clear_objects()\n",
    "alice = alice.clear_objects()\n",
    "secure_worker = secure_worker.clear_objects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create dataset for simple linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Toy Dataset\n",
    "data = th.tensor([[0,0],[0,1],[1,0],[1,1.]], requires_grad=True)\n",
    "target = th.tensor([[0],[0],[1],[1.]], requires_grad=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pointers to training data on each worker by\n",
    "# sending some training data to bob and alice\n",
    "# bob data\n",
    "bobs_data = data[0:2].send(bob)\n",
    "bobs_target = target[0:2].send(bob)\n",
    "# alice data\n",
    "alices_data = data[2:].send(alice)\n",
    "alices_target = target[2:].send(alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize toy model\n",
    "model = nn.Linear(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### securing federated learning\n",
    "\n",
    "* we want to have two different model that we want to send to each of the workers because we want to be averaged\n",
    "* we need two optimizers: one for bob and one for parameters of alice's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the model and send it to bob\n",
    "alices_model = model.copy().send(alice)\n",
    "\n",
    "alices_opt = optim.SGD(params=alices_model.parameters(), lr=0.1)\n",
    "\n",
    "# for alice\n",
    "# zero out the gradients\n",
    "alices_model.zero_grad()\n",
    "# make prediction\n",
    "alices_pred = alices_model(alices_data)\n",
    "# calculate the loss with SME \n",
    "alices_loss = ((alices_pred - alices_target)**2).sum()\n",
    "# backpropagate\n",
    "alices_loss.backward()\n",
    "# make an optimizer step to update the weights \n",
    "alices_opt.step()\n",
    "# get the loss\n",
    "alices_loss = alices_loss.get().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the model and send it to bob\n",
    "bobs_model = model.copy().send(bob)\n",
    "# initialize the optimizers\n",
    "bobs_opt = optim.SGD(params=bobs_model.parameters(), lr=0.1)\n",
    "\n",
    "bobs_model.zero_grad()\n",
    "# make prediction\n",
    "bobs_pred = bobs_model(bobs_data)\n",
    "# calculate the loss with SME \n",
    "bobs_loss = ((bobs_pred - bobs_target)**2).sum()\n",
    "# backpropagate\n",
    "bobs_loss.backward()\n",
    "# make an optimizer step to update the weights \n",
    "bobs_opt.step()\n",
    "# get the loss\n",
    "bobs_loss = bobs_loss.get().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# share the models useing secret sharing\n",
    "alices_model_info = alices_model.fix_precision().share(alice, bob, \n",
    "                                                       crypto_provider=secure_worker)\n",
    "bobs_model_info = bobs_model.fix_precision().share(alice, bob, \n",
    "                                                   crypto_provider=secure_worker)\n",
    "# get the weights\n",
    "alices_weights = alices_model_info.weight.get()\n",
    "bobs_weights = bobs_model_info.weight.get()\n",
    "\n",
    "# get the weights data\n",
    "bobs_weights_data = bobs_weights.child.get().float_precision()\n",
    "alices_weights_data = alices_weights.child.get().float_precision() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the bias\n",
    "alices_bias = alices_model_info.bias.get()\n",
    "bobs_bias = bobs_model_info.bias.get()\n",
    "\n",
    "# get the bias data\n",
    "alices_bias_data = alices_bias.child.get().float_precision() \n",
    "bobs_bias_data = bobs_bias.child.get().float_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    # average the models\n",
    "    model.weight.set_((bobs_weights_data + alices_weights_data)/2)\n",
    "    model.bias.set_((bobs_bias_data + alices_bias_data)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob:tensor(0.0291) Alice:tensor(0.0121)\n",
      "Bob:tensor(0.0084) Alice:tensor(0.0019)\n",
      "Bob:tensor(0.0030) Alice:tensor(0.0002)\n",
      "Bob:tensor(0.0013) Alice:tensor(1.3129e-06)\n",
      "Bob:tensor(0.0007) Alice:tensor(1.4935e-05)\n",
      "Bob:tensor(0.0004) Alice:tensor(2.8330e-05)\n",
      "Bob:tensor(0.0003) Alice:tensor(3.0401e-05)\n",
      "Bob:tensor(0.0002) Alice:tensor(2.8501e-05)\n",
      "Bob:tensor(0.0001) Alice:tensor(2.3376e-05)\n",
      "Bob:tensor(0.0001) Alice:tensor(1.8576e-05)\n"
     ]
    }
   ],
   "source": [
    "for round_iter in range(10):\n",
    "\n",
    "    # copy the model and send it to bob\n",
    "    bobs_model = model.copy().send(bob)\n",
    "    alices_model = model.copy().send(alice)\n",
    "\n",
    "    # initialize the optimizers\n",
    "    bobs_opt = optim.SGD(params=bobs_model.parameters(), lr=0.1)\n",
    "    alices_opt = optim.SGD(params=alices_model.parameters(), lr=0.1)\n",
    "\n",
    "    # training the models and average the models\n",
    "\n",
    "    for i in range(10):\n",
    "\n",
    "        # for bob\n",
    "        # zero out the gradients\n",
    "        bobs_model.zero_grad()\n",
    "        # make prediction\n",
    "        bobs_pred = bobs_model(bobs_data)\n",
    "        # calculate the loss with SME \n",
    "        bobs_loss = ((bobs_pred - bobs_target)**2).sum()\n",
    "        # backpropagate\n",
    "        bobs_loss.backward()\n",
    "        # make an optimizer step to update the weights \n",
    "        bobs_opt.step()\n",
    "        # get the loss\n",
    "        bobs_loss = bobs_loss.get().data\n",
    "        #print(bobs_loss)\n",
    "\n",
    "        # for alice\n",
    "        # zero out the gradients\n",
    "        alices_model.zero_grad()\n",
    "        # make prediction\n",
    "        alices_pred = alices_model(alices_data)\n",
    "        # calculate the loss with SME \n",
    "        alices_loss = ((alices_pred - alices_target)**2).sum()\n",
    "        # backpropagate\n",
    "        alices_loss.backward()\n",
    "        # make an optimizer step to update the weights \n",
    "        alices_opt.step()\n",
    "        # get the loss\n",
    "        alices_loss = alices_loss.get().data\n",
    "        #print(alices_loss)\n",
    "\n",
    "\n",
    "    # share the models using secret sharing\n",
    "    alices_model_info = alices_model.fix_precision().share(alice, bob, \n",
    "                                                           crypto_provider=secure_worker)\n",
    "    bobs_model_info = bobs_model.fix_precision().share(alice, bob, \n",
    "                                                       crypto_provider=secure_worker)\n",
    "    # get the weights\n",
    "    alices_weights = alices_model_info.weight.get()\n",
    "    bobs_weights = bobs_model_info.weight.get()\n",
    "\n",
    "    # get the weights data\n",
    "    bobs_weights_data = bobs_weights.child.get().float_precision()\n",
    "    alices_weights_data = alices_weights.child.get().float_precision() \n",
    "    \n",
    "    # get the bias\n",
    "    alices_bias = alices_model_info.bias.get()\n",
    "    bobs_bias = bobs_model_info.bias.get()\n",
    "\n",
    "    # get the bias data\n",
    "    alices_bias_data = alices_bias.child.get().float_precision() \n",
    "    bobs_bias_data = bobs_bias.child.get().float_precision()\n",
    "\n",
    "    with th.no_grad():\n",
    "        # average the models\n",
    "        model.weight.set_((bobs_weights_data + alices_weights_data)/2)\n",
    "        model.bias.set_((bobs_bias_data + alices_bias_data)/2)\n",
    "        \n",
    "    secure_worker.clear_objects()\n",
    "    \n",
    "    print(\"Bob:\" + str(bobs_loss) + \" Alice:\" + str(alices_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
